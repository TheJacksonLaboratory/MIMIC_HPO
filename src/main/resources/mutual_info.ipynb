{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "mf_module_path = os.path.abspath(os.path.join('../python'))\n",
    "if mf_module_path not in sys.path:\n",
    "    sys.path.append(mf_module_path)\n",
    "import mf\n",
    "import mf_random\n",
    "import hpoutil\n",
    "import networkx\n",
    "import obonet\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydb = mysql.connector.connect(host='localhost',\n",
    "                               user='mimicuser',\n",
    "                               passwd='mimic',\n",
    "                               database='mimiciiiv13',\n",
    "                              auth_plugin='mysql_native_password')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First approach to query mysql from python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor = mydb.cursor(buffered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that MySQL connection works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ITEMID</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>VALUENUM</th>\n",
       "      <th>VALUEUOM</th>\n",
       "      <th>FLAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>51143</td>\n",
       "      <td>2138-07-17 20:48:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>%</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>51144</td>\n",
       "      <td>2138-07-17 20:48:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>%</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>51146</td>\n",
       "      <td>2138-07-17 20:48:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>%</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>51200</td>\n",
       "      <td>2138-07-17 20:48:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>%</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>51221</td>\n",
       "      <td>2138-07-17 20:48:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>%</td>\n",
       "      <td>abnormal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ROW_ID  SUBJECT_ID  HADM_ID  ITEMID           CHARTTIME  VALUE  VALUENUM  \\\n",
       "0       1           2   163353   51143 2138-07-17 20:48:00      0       0.0   \n",
       "1       2           2   163353   51144 2138-07-17 20:48:00      0       0.0   \n",
       "2       3           2   163353   51146 2138-07-17 20:48:00      0       0.0   \n",
       "3       4           2   163353   51200 2138-07-17 20:48:00      0       0.0   \n",
       "4       5           2   163353   51221 2138-07-17 20:48:00      0       0.0   \n",
       "\n",
       "  VALUEUOM      FLAG  \n",
       "0        %      None  \n",
       "1        %      None  \n",
       "2        %      None  \n",
       "3        %      None  \n",
       "4        %  abnormal  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_sql_query(\"SELECT * FROM LABEVENTS LIMIT 5;\", mydb)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we are going to test the mutual information idea:\n",
    "\n",
    "- create a temporary table that combines LABEVENTS(SUBJECT_ID, ITEMID, CHARTTIME), Lab2Hpo(NEGATED, MAP_TO), D_LABEVENTS(LOINC)\n",
    "- define patient phenotype: having a phenotype > 3 times \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a small dataset of 100 encounters (88 distinct patient) for testing with the following code:\n",
    "\n",
    "\"create table test as (with adm as (select distinct subject_id, hadm_id from admissions limit 100) select combined.* from adm left join combined on adm.subject_id = combined.subject_id and adm.hadm_id = combined.hadm_id);\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## EDA before analyzing synergy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some questions we need to further explore. \n",
    "\n",
    "* How many times did each patient get admitted into ICU?\n",
    "\n",
    "From the following query, we can see that about 84% patients were admitted only once, 11% were admitted twice, and the rest 5% were admitted three times or more. We could just focus on the 84%. This will make our task easier as we do not need to worry about effects from other admissions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adm_time</th>\n",
       "      <th>patient_n</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>38983</td>\n",
       "      <td>0.837984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5160</td>\n",
       "      <td>0.110920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1342</td>\n",
       "      <td>0.028848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>508</td>\n",
       "      <td>0.010920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>246</td>\n",
       "      <td>0.005288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>113</td>\n",
       "      <td>0.002429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>51</td>\n",
       "      <td>0.001096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>0.000666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>0.000559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   adm_time  patient_n   percent\n",
       "0         1      38983  0.837984\n",
       "1         2       5160  0.110920\n",
       "2         3       1342  0.028848\n",
       "3         4        508  0.010920\n",
       "4         5        246  0.005288\n",
       "5         6        113  0.002429\n",
       "6         7         51  0.001096\n",
       "7         8         31  0.000666\n",
       "8         9         26  0.000559\n",
       "9        10         14  0.000301"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admissions_per_patient = pd.read_sql_query(\"WITH adm_per_patient AS \\\n",
    "    (SELECT count(*) as adm_time FROM ADMISSIONS GROUP BY SUBJECT_ID) \\\n",
    "    SELECT adm_time, count(*) as patient_n FROM adm_per_patient GROUP BY adm_time ORDER BY patient_n DESC\", mydb)\n",
    "admissions_per_patient['percent'] = admissions_per_patient.patient_n / np.sum(admissions_per_patient.patient_n)\n",
    "admissions_per_patient[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How many abnormal phenotypes does each patient have at each admission?\n",
    "\n",
    "From the histogram, we can see that most patients have ~20 abnormal phenotypes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-30 17:52:12,873 | ERROR : An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line string', (1, 3))\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-562d0bf65e66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mLabHpo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEGATED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabHpo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAP_TO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mFROM\u001b[0m \u001b[0mLABEVENTS\u001b[0m \u001b[0mJOIN\u001b[0m \u001b[0mLabHpo\u001b[0m \u001b[0mon\u001b[0m \u001b[0mLABEVENTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mROW_ID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabHpo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mROW_ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m    ''')\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CREATE INDEX combined_negated ON combined (NEGATED)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/mysql/connector/cursor.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, operation, params, multi)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmd_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstmt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterfaceError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_have_next_result\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=W0212\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/mysql/connector/connection.py\u001b[0m in \u001b[0;36mcmd_query\u001b[0;34m(self, query, raw, buffered, raw_as_string)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_cmd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mServerCmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUERY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_have_next_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/mysql/connector/connection.py\u001b[0m in \u001b[0;36m_send_cmd\u001b[0;34m(self, command, argument, packet_number, packet, expect_response, compressed_packet_number)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexpect_response\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msend_empty_packet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/mysql/connector/network.py\u001b[0m in \u001b[0;36mrecv_plain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mpacket_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mpacket_len\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m                 \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpacket_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterfaceError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2013\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m    985\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m                     self.__class__)\n\u001b[0;32m--> 987\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cursor.execute(\"DROP TEMPORARY TABLE IF EXISTS combined\")\n",
    "cursor.execute('''\n",
    "        CREATE TEMPORARY TABLE IF NOT EXISTS combined AS \n",
    "        SELECT LABEVENTS.ROW_ID, LABEVENTS.SUBJECT_ID, LABEVENTS.HADM_ID, LABEVENTS.ITEMID, LABEVENTS.CHARTTIME, \n",
    "        LabHpo.NEGATED, LabHpo.MAP_TO \n",
    "        FROM LABEVENTS JOIN LabHpo on LABEVENTS.ROW_ID = LabHpo.ROW_ID\n",
    "   ''')\n",
    "cursor.execute(\"CREATE INDEX combined_negated ON combined (NEGATED)\")\n",
    "\n",
    "abnormPerPatientAdm = pd.read_sql_query(\"WITH abnormal_per_patient_adm AS \\\n",
    "(SELECT SUBJECT_ID, HADM_ID, count(*) AS abnormal_n \\\n",
    "FROM combined WHERE NEGATED = 'F' GROUP BY SUBJECT_ID, HADM_ID) \\\n",
    "SELECT abnormal_n, count(*) AS n FROM abnormal_per_patient_adm GROUP BY abnormal_n ORDER BY n DESC \", mydb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormPerPatientAdm.loc[abnormPerPatientAdm.n > 500, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way is to plot this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abnormalities_per_patient_adm = pd.read_sql_query(\"SELECT SUBJECT_ID, HADM_ID, count(*) AS abnormal_n \\\n",
    "FROM combined WHERE NEGATED = 'F' GROUP BY SUBJECT_ID, HADM_ID\", mydb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormalities_per_patient_adm.abnormal_n.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = pd.cut(abnormalities_per_patient_adm.abnormal_n, bins = [-1, 0, 25, 50, 75, 100, 125, 150, 175, 200, 3000, 10000])\n",
    "hist_data = abnormalities_per_patient_adm.groupby(bins).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(211)\n",
    "ax.hist(abnormalities_per_patient_adm.abnormal_n, \n",
    "        bins = np.arange(0, np.max(abnormalities_per_patient_adm.abnormal_n), 50))\n",
    "plt.title('abnormal phenotypes per patient-admission')\n",
    "ax = fig.add_subplot(212)\n",
    "ax.hist(abnormalities_per_patient_adm.abnormal_n, \n",
    "        bins = np.arange(-9, np.max(abnormalities_per_patient_adm.abnormal_n), 10))\n",
    "plt.xlim((-20, 200))\n",
    "plt.title('abnormal phenotypes per patient-admission')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How to assign that a patient has a phenotype at one admission?\n",
    "\n",
    "We look at how many times does each abnormal phenotype occur for each patient admission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query(\"SELECT SUBJECT_ID, HADM_ID, MAP_TO \\\n",
    "    FROM combined \\\n",
    "    WHERE NEGATED = 'F' \\\n",
    "    limit 5\", mydb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_occurrence = pd.read_sql_query(\"SELECT SUBJECT_ID, HADM_ID, MAP_TO, count(*) as n \\\n",
    "    FROM combined \\\n",
    "    WHERE NEGATED = 'F' \\\n",
    "    GROUP BY SUBJECT_ID, HADM_ID, MAP_TO\", mydb)\n",
    "abnormal_occurrence.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an extreme case where a patient had mapped to 906 Hypercapnia HP:0012416"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_occurrence[abnormal_occurrence.n == 906]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(211)\n",
    "ax.hist(abnormal_occurrence.n, \n",
    "        bins = np.arange(0, np.max(abnormal_occurrence.n), 50))\n",
    "plt.yscale('log')\n",
    "plt.title('distribution of abnormal phenotype duplication times')\n",
    "ax = fig.add_subplot(212)\n",
    "ax.hist(abnormal_occurrence.n, \n",
    "        bins = np.arange(0, np.max(abnormal_occurrence.n), 1))\n",
    "plt.xlim((-2, 20))\n",
    "plt.xlabel('duplication times of each phenotype')\n",
    "plt.title('distribution of abnormal phenotype duplication times')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, we conclude that the majority of HPO only occurred once for every patient at each visit. There are some extreme cases where one patient was assigned with hundreds of HPO terms in one admission. They might indicate patients that had lone stay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stay_duration = pd.read_sql_query(\"SELECT \\\n",
    "    SUBJECT_ID, HADM_ID, DISCHTIME, ADMITTIME, CEILING(TIME_TO_SEC(TIMEDIFF(DISCHTIME, ADMITTIME))/(3600 * 24)) as days \\\n",
    "    FROM ADMISSIONS\", mydb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stay_duration.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(211)\n",
    "ax.hist(stay_duration.days, \n",
    "        bins = np.arange(0, np.max(stay_duration.days), 4))\n",
    "ax = fig.add_subplot(212)\n",
    "ax.hist(stay_duration.days, \n",
    "        bins = np.arange(0, np.max(stay_duration.days), 1))\n",
    "plt.xlim((-2, 35))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most patients stayed in ICU for around 5 days. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the common diagnosis at each admission?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diagnosis_count = pd.read_sql_query(\"SELECT SUBJECT_ID, HADM_ID, \\\n",
    "    CASE \\\n",
    "    WHEN(ICD9_CODE LIKE 'V%') THEN SUBSTRING(ICD9_CODE, 1, 3) \\\n",
    "    WHEN(ICD9_CODE LIKE 'E%') THEN SUBSTRING(ICD9_CODE, 1, 4) \\\n",
    "    ELSE SUBSTRING(ICD9_CODE, 1, 3) END AS ICD9 \\\n",
    "    FROM DIAGNOSES_ICD\", mydb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_count.drop_duplicates().groupby('ICD9').size().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_count.head(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_count.ICD9.value_counts().head(n = 40).plot(kind='bar')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top diagnosis codes are:\n",
    "\n",
    "* 401 Essential hypertension\n",
    "* 427 Cardiac dysrhythmias\n",
    "* 428 Heart failure\n",
    "* 276 Disorders of fluid, electrolyte, and acid-base balance\n",
    "* 414 Other forms of chronic ischemic heart disease\n",
    "* 272 Disorders of lipoid metabolism\n",
    "* 518 Other diseases of lung\n",
    "* 285 Other and unspecified anemias\n",
    "* 584 Acute renal failure\n",
    "* V45 Other postprocedural states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1 --SKIP TO Method 3\n",
    "\n",
    "Now we can try to determine the mutual information content for one diagnosis: Heart failure. Our algorithm is like the following:\n",
    "* Group patient+admissions into two groups: having code 428 and not having it;\n",
    "* For each group, count total patient+admission, count patient+admission having Phenotype X or not. \n",
    "* For each group, count total patient+admission, count patient+admission having Phenotype Y or not.\n",
    "* For each group, count total patient+admission, count patient+admission having Phenotype XY, X NOT Y, Y NOT X, NOT X NOT Y. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at what are the commonly seen abnormalities for heart failure patients. We count each phenotype once if they happen multiple times during one admission for each patient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pheno_heart_failure = pd.read_sql_query(\"WITH p AS (SELECT DISTINCT SUBJECT_ID, HADM_ID FROM DIAGNOSES_ICD WHERE ICD9_CODE LIKE '428%'), \\\n",
    "    c AS (SELECT DISTINCT SUBJECT_ID, HADM_ID, MAP_TO FROM combined WHERE NEGATED = 'F') \\\n",
    "    SELECT DISTINCT c.MAP_TO, count(DISTINCT c.SUBJECT_ID, c.HADM_ID, c.MAP_TO) AS count \\\n",
    "FROM c JOIN p ON c.SUBJECT_ID = p.SUBJECT_ID AND c.HADM_ID = p.HADM_ID GROUP BY c.MAP_TO ORDER BY count DESC\", mydb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_heart_failure.head(n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows that the most frequent top 6 abnormal phenotypes for patients diagnosed with heart failure: \n",
    "\n",
    "* HP:0003074 Hyperglycemia\n",
    "* HP:0001943 Hypoglycemia\n",
    "* HP:0031851 Reduced hematocrit\n",
    "* HP:0003138 Increased blood urea nitrogen\n",
    "* HP:0020062 Decreased hemoglobin concentration\n",
    "* HP:0002901 Hypocalcemia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate I(HP:0003074 | heart failure). \n",
    "To do this, we need a table for (subject, admission, hasPhenotype) for HP:0003074; then another table for (subject, admission, hasDiagnosis) for ICD9:428\\*. Then we can join them together on (subject, admission). Finally, calculate frequencies of (phenotype | diagnosis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query(\"SELECT DISTINCT SUBJECT_ID, HADM_ID, IF (ICD9_CODE LIKE '428%', 'Y', 'N') AS hasDiagnosis FROM DIAGNOSES_ICD LIMIT 10\", mydb)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query(\"SELECT SUBJECT_ID, HADM_ID, IF(SUM(MAP_TO = 'HP:0003074') > 0, 'Y', 'N') AS hasPhenotype \\\n",
    "    FROM combined \\\n",
    "    where NEGATED = 'F' AND HADM_ID IS NOT NULL \\\n",
    "    GROUP BY SUBJECT_ID, HADM_ID \\\n",
    "    LIMIT 10\", mydb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query(\"SELECT * FROM combined LIMIT 10\", mydb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p1 = pd.read_sql_query(\"WITH \\\n",
    "    l AS \\\n",
    "        (SELECT DISTINCT SUBJECT_ID, HADM_ID, IF (ICD9_CODE LIKE '428%', 'Y', 'N') AS hasDiagnosis \\\n",
    "        FROM DIAGNOSES_ICD), \\\n",
    "    r AS \\\n",
    "        (SELECT SUBJECT_ID, HADM_ID, \\\n",
    "        IF(SUM(MAP_TO = 'HP:0003074') > 0, 'Y', 'N') AS hasPhenotype1, \\\n",
    "        IF(SUM(MAP_TO = 'HP:0002850') > 0, 'Y', 'N') AS hasPhenotype2 \\\n",
    "        FROM combined \\\n",
    "        where NEGATED = 'F' AND HADM_ID IS NOT NULL \\\n",
    "        GROUP BY SUBJECT_ID, HADM_ID) \\\n",
    "    SELECT l.SUBJECT_ID, l.HADM_ID, l.hasDiagnosis, r.hasPhenotype1, r.hasPhenotype2 \\\n",
    "    FROM l LEFT JOIN r ON l.SUBJECT_ID = r.SUBJECT_ID AND l.HADM_ID = r.HADM_ID\", mydb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we have the phenotype-diagnosis table, we can compute I(P|D).\n",
    "\n",
    "Calculate p(P = 'Y'), p(D = 'Y'), \n",
    "p(P = 'Y' and D = 'Y'), p(P = 'N' and D = 'Y'), p(P = 'Y' and D = 'N'), p(P = 'N' and D = 'N')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mutual_information(dataset, phenotype):\n",
    "    \"\"\"\n",
    "    pPYDY = sum(np.logical_and(p == 'Y', d == 'Y')) / SIZE\n",
    "    pPNDY = sum(np.logical_and(p == 'N', d == 'Y')) / SIZE\n",
    "    pPYDN = sum(np.logical_and(p == 'Y', d == 'N')) / SIZE\n",
    "    pPNDN = sum(np.logical_and(p == 'N', d == 'N')) / SIZE\n",
    "    \"\"\"\n",
    "    p = dataset[phenotype]\n",
    "    d = dataset['hasDiagnosis']\n",
    "    SIZE = len(dataset)\n",
    "    pPY = sum(p == 'Y') / SIZE\n",
    "    pPN = 1 - pPY\n",
    "    pDY = sum(d == 'Y') / SIZE\n",
    "    pDN = 1 - pDY\n",
    "    temp = dataset.groupby([phenotype, 'hasDiagnosis']).size()\n",
    "    if len(temp) < 4:\n",
    "        return (99.99)\n",
    "    pPYDY = temp.loc['Y', 'Y'] / SIZE\n",
    "    pPNDY = temp.loc['N', 'Y'] / SIZE\n",
    "    pPYDN = temp.loc['Y', 'N'] / SIZE\n",
    "    pPNDN = temp.loc['N', 'N'] / SIZE\n",
    "    \n",
    "    I = pPYDY * np.log2(pPYDY / (pPY * pDY)) + \\\n",
    "        pPNDY * np.log2(pPNDY / (pPN * pDY)) + \\\n",
    "        pPYDN * np.log2(pPYDN / (pPY * pDN)) + \\\n",
    "        pPNDN * np.log2(pPNDN / (pPN * pDN))\n",
    "        \n",
    "    return(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ip1d = mutual_information(p1, phenotype = 'hasPhenotype1')\n",
    "Ip2d = mutual_information(p1, phenotype = 'hasPhenotype2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Ip1d)\n",
    "print(Ip2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mutual_information2(dataset):\n",
    "    p1 = dataset['hasPhenotype1']\n",
    "    p2 = dataset['hasPhenotype2']\n",
    "    d = dataset['hasDiagnosis']\n",
    "    SIZE = len(dataset)\n",
    "    p1Y = sum(p1 == 'Y') / SIZE\n",
    "    p2Y = sum(p2 == 'Y') / SIZE\n",
    "    pDY = sum(d == 'Y') / SIZE\n",
    "    pDN = 1 - pDY\n",
    "    \n",
    "    \"\"\"    \n",
    "    pPYY = sum(np.logical_and(p1 == 'Y', p2 == 'Y')) / SIZE\n",
    "    pPYN = sum(np.logical_and(p1 == 'Y', p2 == 'N')) / SIZE\n",
    "    pPNY = sum(np.logical_and(p1 == 'N', p2 == 'Y')) / SIZE\n",
    "    pPNN = sum(np.logical_and(p1 == 'N', p2 == 'N')) / SIZE\n",
    "    \"\"\"\n",
    "    \n",
    "    temp1 = dataset.groupby(['hasPhenotype1', 'hasPhenotype2']).size()\n",
    "    if len(temp1) < 4:\n",
    "        print(temp1)\n",
    "        return (-99.99)\n",
    "    pPYY = temp1.loc(axis=0)['Y', 'Y'] / SIZE\n",
    "    pPYN = temp1.loc(axis=0)['Y', 'N'] / SIZE\n",
    "    pPNY = temp1.loc(axis=0)['N', 'Y'] / SIZE\n",
    "    pPNN = temp1.loc(axis=0)['N', 'N'] / SIZE\n",
    "\n",
    "    \"\"\"\n",
    "    pPYYDY = sum(np.logical_and(np.logical_and(p1 == 'Y', p2 == 'Y'), d == 'Y')) / SIZE\n",
    "    pPYNDY = sum(np.logical_and(np.logical_and(p1 == 'Y', p2 == 'N'), d == 'Y')) / SIZE\n",
    "    pPNYDY = sum(np.logical_and(np.logical_and(p1 == 'N', p2 == 'Y'), d == 'Y')) / SIZE\n",
    "    pPNNDY = sum(np.logical_and(np.logical_and(p1 == 'N', p2 == 'N'), d == 'Y')) / SIZE\n",
    "    pPYYDN = sum(np.logical_and(np.logical_and(p1 == 'Y', p2 == 'Y'), d == 'N')) / SIZE\n",
    "    pPYNDN = sum(np.logical_and(np.logical_and(p1 == 'Y', p2 == 'N'), d == 'N')) / SIZE\n",
    "    pPNYDN = sum(np.logical_and(np.logical_and(p1 == 'N', p2 == 'Y'), d == 'N')) / SIZE\n",
    "    pPNNDN = sum(np.logical_and(np.logical_and(p1 == 'N', p2 == 'N'), d == 'N')) / SIZE\n",
    "    \"\"\"\n",
    "    \n",
    "    temp2 = dataset.groupby(['hasPhenotype1', 'hasPhenotype2', 'hasDiagnosis']).size()\n",
    "    if len(temp2) != 8:\n",
    "        print(temp2)\n",
    "        return (-99.99)\n",
    "    pPYYDY = temp2.loc(axis=0)['Y', 'Y', 'Y'] / SIZE\n",
    "    pPYNDY = temp2.loc(axis=0)['Y', 'N', 'Y'] / SIZE\n",
    "    pPNYDY = temp2.loc(axis=0)['N', 'Y', 'Y'] / SIZE\n",
    "    pPNNDY = temp2.loc(axis=0)['N', 'N', 'Y'] / SIZE\n",
    "    pPYYDN = temp2.loc(axis=0)['Y', 'Y', 'N'] / SIZE\n",
    "    pPYNDN = temp2.loc(axis=0)['Y', 'N', 'N'] / SIZE\n",
    "    pPNYDN = temp2.loc(axis=0)['N', 'Y', 'N'] / SIZE\n",
    "    pPNNDN = temp2.loc(axis=0)['N', 'N', 'N'] / SIZE\n",
    "    \n",
    "    I = pPYYDY * np.log2(pPYYDY / (pPYY * pDY)) + \\\n",
    "        pPYNDY * np.log2(pPYNDY / (pPYN * pDY)) + \\\n",
    "        pPNYDY * np.log2(pPNYDY / (pPNY * pDY)) + \\\n",
    "        pPNNDY * np.log2(pPNNDY / (pPNN * pDY)) + \\\n",
    "        pPYYDN * np.log2(pPYYDN / (pPYY * pDN)) + \\\n",
    "        pPYNDN * np.log2(pPYNDN / (pPYN * pDN)) + \\\n",
    "        pPNYDN * np.log2(pPNYDN / (pPNY * pDN)) + \\\n",
    "        pPNNDN * np.log2(pPNNDN / (pPNN * pDN))\n",
    "        \n",
    "    return(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ip1p2 = mutual_information2(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "syn = Ip1p2 - Ip1d - Ip2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine everything together:\n",
    "\n",
    "    for every diagnosis code,\n",
    "        (ignore diagnosis codes with all 0 or all 1)\n",
    "        for every pair of phenotypes,\n",
    "        (ignore phenotypes with all 0 or all 1)\n",
    "            calculate synergy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diagnosisSet = diagnosis_count.drop_duplicates().groupby('ICD9').size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diagnosisSet.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Method 1 implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_all_brutal():\n",
    "    #for diagnosis in diagnosisSet.keys():\n",
    "    for diagnosis in ['428']:\n",
    "        if (diagnosisSet[diagnosis] > 10000):\n",
    "            queryString = \"WITH p AS (SELECT DISTINCT SUBJECT_ID, HADM_ID FROM DIAGNOSES_ICD WHERE ICD9_CODE LIKE '{}%'), \\\n",
    "        c AS (SELECT DISTINCT SUBJECT_ID, HADM_ID, MAP_TO FROM combined WHERE NEGATED = 'F') \\\n",
    "        SELECT DISTINCT c.MAP_TO, count(DISTINCT c.SUBJECT_ID, c.HADM_ID, c.MAP_TO) AS count \\\n",
    "    FROM c JOIN p ON c.SUBJECT_ID = p.SUBJECT_ID AND c.HADM_ID = p.HADM_ID GROUP BY c.MAP_TO ORDER BY count DESC\".format(diagnosis)\n",
    "            phenolist = pd.read_sql_query(queryString, mydb)\n",
    "            phenoSet = phenolist[phenolist['count'] > 100].MAP_TO\n",
    "            for i in np.arange(len(phenoSet)):\n",
    "                phenotype1 = phenoSet[i]\n",
    "                for j in np.arange(i + 1, len(phenoSet)):\n",
    "                    phenotype2 = phenoSet[j]\n",
    "                    queryString = \"WITH \\\n",
    "                        l AS \\\n",
    "                            (SELECT DISTINCT SUBJECT_ID, HADM_ID, IF (ICD9_CODE LIKE '{}%', 'Y', 'N') AS hasDiagnosis \\\n",
    "                            FROM DIAGNOSES_ICD), \\\n",
    "                        r AS \\\n",
    "                            (SELECT SUBJECT_ID, HADM_ID, \\\n",
    "                            IF(SUM(MAP_TO = '{}') > 0, 'Y', 'N') AS hasPhenotype1, \\\n",
    "                            IF(SUM(MAP_TO = '{}') > 0, 'Y', 'N') AS hasPhenotype2 \\\n",
    "                            FROM combined \\\n",
    "                            where NEGATED = 'F' AND HADM_ID IS NOT NULL \\\n",
    "                            GROUP BY SUBJECT_ID, HADM_ID) \\\n",
    "                        SELECT l.SUBJECT_ID, l.HADM_ID, l.hasDiagnosis, r.hasPhenotype1, r.hasPhenotype2 \\\n",
    "                        FROM l LEFT JOIN r ON l.SUBJECT_ID = r.SUBJECT_ID AND l.HADM_ID = r.HADM_ID\".format(diagnosis, phenotype1, phenotype2)\n",
    "                    p1 = pd.read_sql_query(queryString, mydb)\n",
    "                    p1 = p1.fillna('N')\n",
    "                    if (len(p1.hasPhenotype1.unique()) == 2) and (len(p1.hasPhenotype2.unique()) == 2):\n",
    "                        Ip1d = mutual_information(p1, phenotype = 'hasPhenotype1')\n",
    "                        Ip2d = mutual_information(p1, phenotype = 'hasPhenotype2')\n",
    "                        Ip1p2 = mutual_information2(p1)\n",
    "                        syn = Ip1p2 - Ip1d - Ip2d\n",
    "                        print(\"diagnosis = {} phenotype1 = {}, phenotype2 = {}, syn = {}\".format(diagnosis, phenotype1, phenotype2, syn))\n",
    "                        writer.write(\"diagnosis = {} phenotype1 = {}, phenotype2 = {}, syn = {}\\n\".format(diagnosis, phenotype1, phenotype2, syn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a writer \n",
    "#writer = open('result.txt', 'w+')\n",
    "# prepare MySQL\n",
    "start = datetime.datetime.now()\n",
    "#cursor.execute(\"DROP TEMPORARY TABLE IF EXISTS combined\")\n",
    "#cursor.execute('''\n",
    "#        CREATE TEMPORARY TABLE combined AS \n",
    "#        SELECT LABEVENTS.ROW_ID, LABEVENTS.SUBJECT_ID, LABEVENTS.HADM_ID, LABEVENTS.ITEMID, LABEVENTS.CHARTTIME, \n",
    "#        LabHpo.NEGATED, LabHpo.MAP_TO \n",
    "#        FROM LABEVENTS JOIN LabHpo on LABEVENTS.ROW_ID = LabHpo.ROW_ID\n",
    "#    ''')\n",
    "#cursor.execute(\"CREATE INDEX combined_negated ON combined (NEGATED)\")\n",
    "end = datetime.datetime.now()\n",
    "print('running time: {}s'.format((end - start).total_seconds()))\n",
    "# run the iteration\n",
    "#iterate_all_brutal()\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above approach has two major issues: \n",
    "\n",
    "1. It computes the mutual information between diagnosis and each phenotype for each phenotype pair. We should have saved them so that we compute them only once.\n",
    "2. It transfers data from MySQL to Pandas. We could use MySQL to do all the work\n",
    "3. It did not take full advantage of MySQL index.\n",
    "\n",
    "Therefore, we come up with the following approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Method 2 - SKIP TO Method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note this takes a long time and large disk size to run\n",
    "def iterate_all_brutal2():\n",
    "    start = datetime.datetime.now()\n",
    "    queryString = '''\n",
    "        WITH\n",
    "\n",
    "            combined AS(\n",
    "                SELECT\n",
    "                    LABEVENTS.ROW_ID, LABEVENTS.SUBJECT_ID, LABEVENTS.HADM_ID, LABEVENTS.ITEMID, LABEVENTS.CHARTTIME,\n",
    "                    LabHpo.NEGATED, LabHpo.MAP_TO \n",
    "                FROM \n",
    "                    LABEVENTS JOIN LabHpo on LABEVENTS.ROW_ID = LabHpo.ROW_ID\n",
    "            ),\n",
    "\n",
    "            e AS (SELECT DISTINCT SUBJECT_ID, HADM_ID FROM admissions), \n",
    "            -- all encounters to analysis\n",
    "            p AS (\n",
    "                SELECT \n",
    "                    DISTINCT SUBJECT_ID, HADM_ID, '1' as DIAGNOSIS \n",
    "                FROM DIAGNOSES_ICD \n",
    "                WHERE ICD9_CODE LIKE '{}%'),\n",
    "            -- encounters that had the desired diagnosis\n",
    "            d AS (\n",
    "                SELECT \n",
    "                    e.SUBJECT_ID, e.HADM_ID, IF(p.DIAGNOSIS IS NULL, '0', '1') AS DIAGNOSIS \n",
    "                FROM \n",
    "                    p RIGHT JOIN e ON e.SUBJECT_ID = p.SUBJECT_ID AND e.HADM_ID = p.HADM_ID), \n",
    "            -- diagnosis table: showing whether each encounter had the diagnosis code in research or not\n",
    "            c AS (\n",
    "                SELECT \n",
    "                    DISTINCT SUBJECT_ID, HADM_ID, MAP_TO, '1' AS PRESENT \n",
    "                FROM combined \n",
    "                WHERE NEGATED = 'F'), \n",
    "            -- table of abnormal phenotypes from lab tests\n",
    "            l AS (\n",
    "                SELECT \n",
    "                    DISTINCT c.MAP_TO, count(DISTINCT c.SUBJECT_ID, c.HADM_ID, c.MAP_TO) AS count \n",
    "                FROM \n",
    "                    c JOIN p ON c.SUBJECT_ID = p.SUBJECT_ID AND c.HADM_ID = p.HADM_ID \n",
    "                GROUP BY \n",
    "                    c.MAP_TO \n",
    "                ORDER BY \n",
    "                    count DESC),\n",
    "            -- count how many encounters does a phenotype occur \n",
    "            ls AS (\n",
    "                SELECT \n",
    "                    MAP_TO \n",
    "                FROM l \n",
    "                WHERE count > 5), \n",
    "            -- list of phenotypes to analyze: we limit out analysis to those phenotypes that happened at least N times\n",
    "            hp AS (\n",
    "                SELECT \n",
    "                    e.SUBJECT_ID, e.HADM_ID, ls.MAP_TO \n",
    "                FROM \n",
    "                    e JOIN ls), \n",
    "            -- create a cross product of the encounters * phenotypes: used below\n",
    "            chp AS (\n",
    "                SELECT \n",
    "                    hp.SUBJECT_ID, hp.HADM_ID, hp.MAP_TO, IF(c.PRESENT IS NULL, '0', '1') AS PRESENT \n",
    "                FROM \n",
    "                    c RIGHT JOIN hp ON c.SUBJECT_ID = hp.SUBJECT_ID AND c.HADM_ID = hp.HADM_ID AND c.MAP_TO = hp.MAP_TO), \n",
    "            -- join phenotypes to the encounters * phenotypes table, 1 if a phenotype is present, 0 if a phenotype is not present\n",
    "            s AS (\n",
    "                SELECT \n",
    "                    d.SUBJECT_ID, d.HADM_ID, d.DIAGNOSIS, chp.MAP_TO, chp.PRESENT \n",
    "                FROM \n",
    "                    d RIGHT JOIN chp on d.SUBJECT_ID = chp.SUBJECT_ID AND d.HADM_ID = chp.HADM_ID),\n",
    "            -- joint table that also shows the diagnosis code \n",
    "            -- TODO: join diagnosis while creating hp; ignore this request if we decide to analyze all encounters in one run\n",
    "            z AS (\n",
    "                SELECT \n",
    "                    MAP_TO, PRESENT, DIAGNOSIS, COUNT(*) AS N \n",
    "                FROM \n",
    "                    s \n",
    "                GROUP BY \n",
    "                    MAP_TO, PRESENT, DIAGNOSIS \n",
    "                ORDER BY \n",
    "                    MAP_TO),\n",
    "            -- summary counts for each phenotype, phenotype: 0 or 1, diagnosis: 0, 1\n",
    "            /*\n",
    "            w AS (\n",
    "                SELECT \n",
    "                    s.SUBJECT_ID, s.HADM_ID, s.DIAGNOSIS, s.MAP_TO, s.PRESENT, ls.MAP_TO AS MAP_TO2 \n",
    "                FROM \n",
    "                    s \n",
    "                JOIN ls), -- ORDER BY SUBJECT_ID, HADM_ID, MAP_TO\n",
    "            -- for the joint table s, add cross product of phenotypes so that we have information of phenotype pairs \n",
    "            v AS (\n",
    "                SELECT \n",
    "                    w.*, s.PRESENT AS PRESENT2 \n",
    "                From \n",
    "                    w JOIN s on w.SUBJECT_ID = s.SUBJECT_ID AND w.HADM_ID = s.HADM_ID AND w.MAP_TO2 = s.MAP_TO \n",
    "                    -- ORDER BY SUBJECT_ID, HADM_ID, MAP_TO \n",
    "                ) \n",
    "            -- add value for the second phenotype of each pair\n",
    "            */\n",
    "            v AS (\n",
    "                SELECT \n",
    "                    s1.*, s2.MAP_TO AS MAP_TO2, s2.PRESENT AS PRESENT2\n",
    "                FROM\n",
    "                    s s1 JOIN s s2 WHERE s1.SUBJECT_ID = s2.SUBJECT_ID AND s1.HADM_ID = s2.HADM_ID\n",
    "            )\n",
    "\n",
    "            SELECT \n",
    "                DIAGNOSIS, MAP_TO, PRESENT, MAP_TO2, PRESENT2, count(*) AS N \n",
    "            FROM \n",
    "                v \n",
    "            GROUP BY \n",
    "                DIAGNOSIS, MAP_TO, PRESENT, MAP_TO2, PRESENT2 \n",
    "            -- ORDER BY \n",
    "            --    MAP_TO, MAP_TO2\n",
    "\n",
    "\n",
    "        '''.format('428')\n",
    "\n",
    "\n",
    "    data = pd.read_sql_query(queryString, mydb)\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    print('running time: {}s'.format((end - start).total_seconds()))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above method looks okay, but requires a lot of hard disk space. The reason is that it tries to create many large temporary tables and has to perform a self join of a large table. Therefore, we propose the following method, which works nicely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3 - WORKING !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An improved Algorithm:\n",
    "    \n",
    "    * Create a table with SUBJECT_ID, HADM_ID, DIAGNOSIS, MAP_TO\n",
    "    * Get the set of phenotypes to analyze, put it in python\n",
    "    * Iterate the phenotype set:\n",
    "        For Phenotype 1 in Set:\n",
    "            For Phenotype 2 in Set (After Phenotype 1):\n",
    "                Create a jointed table with (SUBJECT_ID, HADM_ID, DIAGNOSIS, PHENOTYPE1, PHENOTYPE 2)\n",
    "                Count after group by (Diagnosis, Phenotype 1, Phenotype 2), output this table to python (Add name of Phenotype 1 and Phenotype 2)\n",
    "     \n",
    "We will create temporary tables for intermediate results, and also create indices to speed up following operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: rewrite to be backward compatible\n",
    "def diagnosis_set():\n",
    "    \"\"\"Aggregate ICD9 codes with the first three digit and count how many times they appear. \n",
    "    Note this function uses encounters as the unit, meaning a code will counted twice if same patient was \n",
    "    diagnosed again at a later encounter.\"\"\"\n",
    "    diagnosis_count = pd.read_sql_query(\"SELECT SUBJECT_ID, HADM_ID, \\\n",
    "        CASE \\\n",
    "        WHEN(ICD9_CODE LIKE 'V%') THEN SUBSTRING(ICD9_CODE, 1, 3) \\\n",
    "        WHEN(ICD9_CODE LIKE 'E%') THEN SUBSTRING(ICD9_CODE, 1, 4) \\\n",
    "        ELSE SUBSTRING(ICD9_CODE, 1, 3) END AS ICD9 \\\n",
    "        FROM DIAGNOSES_ICD\", mydb)\n",
    "    diagnosisSet = diagnosis_count.drop_duplicates().groupby('ICD9').size().sort_values(ascending=False)\n",
    "    return diagnosisSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diagnosis = diagnosis_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: rewrite to be backward compatible\n",
    "def createAbnormalPhenotypeTable(threshold, include_inferred=True, force_update=True):\n",
    "    \"\"\"\n",
    "    This is the abnormal phenotypes. \n",
    "    @include_inferred whether to include inferred HPO. Default true.\n",
    "    @force_update whether current table, if present, should be forced to update\n",
    "    \"\"\"\n",
    "    if force_update:\n",
    "        cursor.execute('''DROP TABLE IF EXISTS p''')\n",
    "    if include_inferred:\n",
    "        cursor.execute('''\n",
    "                    CREATE TEMPORARY TABLE IF NOT EXISTS p\n",
    "                    WITH abnorm AS (\n",
    "                        SELECT\n",
    "                            LABEVENTS.SUBJECT_ID, LABEVENTS.HADM_ID, LabHpo.MAP_TO\n",
    "                        FROM \n",
    "                            LABEVENTS \n",
    "                        JOIN LabHpo on LABEVENTS.ROW_ID = LabHpo.ROW_ID\n",
    "                        WHERE LabHpo.NEGATED = 'F'\n",
    "                        \n",
    "                        UNION ALL\n",
    "                        \n",
    "                        SELECT \n",
    "                            LABEVENTS.SUBJECT_ID, LABEVENTS.HADM_ID, INFERRED_LABHPO.INFERRED_TO AS MAP_TO \n",
    "                        FROM \n",
    "                            INFERRED_LABHPO \n",
    "                        JOIN \n",
    "                            LABEVENTS ON INFERRED_LABHPO.LABEVENT_ROW_ID = LABEVENTS.ROW_ID\n",
    "                        )\n",
    "                    SELECT SUBJECT_ID, HADM_ID, MAP_TO\n",
    "                    FROM abnorm \n",
    "                    GROUP BY SUBJECT_ID, HADM_ID, MAP_TO\n",
    "                    HAVING COUNT(*) > {}\n",
    "                    -- parameter to control how to define an abnormal phenotype is present.\n",
    "                '''.format(threshold))\n",
    "    else:       \n",
    "        cursor.execute('''\n",
    "                    CREATE TEMPORARY TABLE IF NOT EXISTS p\n",
    "                    WITH abnorm AS (\n",
    "                        SELECT\n",
    "                            LABEVENTS.SUBJECT_ID, LABEVENTS.HADM_ID, LabHpo.MAP_TO\n",
    "                        FROM \n",
    "                            LABEVENTS \n",
    "                        JOIN LabHpo on LABEVENTS.ROW_ID = LabHpo.ROW_ID\n",
    "                        WHERE LabHpo.NEGATED = 'F')\n",
    "                    SELECT SUBJECT_ID, HADM_ID, MAP_TO\n",
    "                    FROM abnorm \n",
    "                    GROUP BY SUBJECT_ID, HADM_ID, MAP_TO\n",
    "                    HAVING COUNT(*) > {}\n",
    "                    -- parameter to control how to define an abnormal phenotype is present.\n",
    "                '''.format(threshold))\n",
    "    cursor.execute('CREATE INDEX p_idx01 ON p (SUBJECT_ID, HADM_ID)')\n",
    "    cursor.execute('CREATE INDEX p_idx02 ON p (MAP_TO);')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "createAbnormalPhenotypeTable(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query('select * from p limit 5', mydb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selectEncounters(force_update=True):\n",
    "    if force_update:\n",
    "        cursor.execute('DROP TEMPORARY TABLE IF EXISTS a')\n",
    "    cursor.execute('''\n",
    "                CREATE TEMPORARY TABLE IF NOT EXISTS a \n",
    "                SELECT \n",
    "                    DISTINCT SUBJECT_ID, HADM_ID \n",
    "                FROM admissions\n",
    "                -- This is admissions that we want to analyze, remove 'LIMIT 100' to analyze all\n",
    "                ''')\n",
    "    return pd.read_sql('SELECT COUNT(*) FROM a', mydb).iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectEncounters(force_update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createDiagnosisTable(diagnosis, force_update=True):\n",
    "    if force_update:\n",
    "        cursor.execute('''DROP TEMPORARY TABLE IF EXISTS j1 ''')\n",
    "    cursor.execute('''\n",
    "                CREATE TEMPORARY TABLE IF NOT EXISTS j1 \n",
    "                WITH \n",
    "                    d AS (\n",
    "                        SELECT \n",
    "                            DISTINCT SUBJECT_ID, HADM_ID, '1' AS DIAGNOSIS\n",
    "                        FROM \n",
    "                            DIAGNOSES_ICD \n",
    "                        WHERE ICD9_CODE LIKE '{}%')\n",
    "                    -- This is encounters with positive diagnosis\n",
    "\n",
    "                SELECT \n",
    "                    a.SUBJECT_ID, a.HADM_ID, IF(d.DIAGNOSIS IS NULL, '0', '1') AS DIAGNOSIS\n",
    "                FROM \n",
    "                    a\n",
    "                LEFT JOIN\n",
    "                    d ON a.SUBJECT_ID = d.SUBJECT_ID AND a.HADM_ID = d.HADM_ID       \n",
    "                /* -- This is the first join for diagnosis (0, or 1) */    \n",
    "                '''.format(diagnosis))\n",
    "    cursor.execute('''CREATE INDEX j1_idx01 ON j1 (SUBJECT_ID, HADM_ID)''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "createDiagnosisTable('428', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query('select * from j1 limit 5', mydb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phenolist(diagnosis):\n",
    "    \"\"\"\n",
    "    After defining whether each patient*encounter has a certain phenotypes, \n",
    "    count how many patient*encounter for each phenotype. \"\"\"\n",
    "    phenolistTable = pd.read_sql_query('''\n",
    "        WITH pd AS(\n",
    "            SELECT p.*\n",
    "            FROM \n",
    "                p JOIN (SELECT \n",
    "                            DISTINCT SUBJECT_ID, HADM_ID, '1' AS DIAGNOSIS\n",
    "                        FROM \n",
    "                            DIAGNOSES_ICD \n",
    "                        WHERE ICD9_CODE LIKE '{}%') AS d\n",
    "                ON p.SUBJECT_ID = d.SUBJECT_ID AND p.HADM_ID = d.HADM_ID)\n",
    "        SELECT \n",
    "            MAP_TO, COUNT(*) AS N\n",
    "        FROM pd\n",
    "        GROUP BY MAP_TO\n",
    "        '''.format(diagnosis), mydb)\n",
    "    #phenolistTable = phenolistTable.set_index('MAP_TO')\n",
    "    return phenolistTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pl = phenolist('428') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "phenoSet = pl[pl['N'] > 1000].reset_index().MAP_TO\n",
    "print(phenoSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def diagnosisAndPhenotype1(phenotype):\n",
    "    cursor.execute('''\n",
    "        DROP TEMPORARY TABLE IF EXISTS p1''')\n",
    "    cursor.execute('''\n",
    "        CREATE TEMPORARY TABLE p1\n",
    "        SELECT j1.*, IF(s.MAP_TO IS NULL, '0', '1') AS PHENOTYPE1\n",
    "        FROM j1 \n",
    "        LEFT JOIN \n",
    "        (SELECT * FROM p WHERE p.MAP_TO = '{}') AS s \n",
    "        ON j1.SUBJECT_ID = s.SUBJECT_ID AND j1.HADM_ID = s.HADM_ID \n",
    "    '''.format(phenotype))\n",
    "    cursor.execute('CREATE INDEX p1_idx01 ON p1 (SUBJECT_ID, HADM_ID)')\n",
    "    # count summary statistics for one phenotype\n",
    "    return pd.read_sql_query('''\n",
    "        SELECT \n",
    "            DIAGNOSIS, PHENOTYPE1 AS PHENOTYPE, COUNT(*) AS N \n",
    "        FROM p1\n",
    "        GROUP BY \n",
    "            DIAGNOSIS, PHENOTYPE1  \n",
    "    ''', mydb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.read_sql_query('''\n",
    "select * from p LIMIT 5\n",
    "''', mydb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def diagnosisAndPhenotype2(phenotype):\n",
    "    result = pd.read_sql_query('''\n",
    "        WITH c AS (\n",
    "        SELECT p1.*, IF(s.MAP_TO IS NULL, '0', '1') AS PHENOTYPE2\n",
    "        FROM p1 \n",
    "        LEFT JOIN (SELECT * FROM p WHERE p.MAP_TO = '{}') AS s \n",
    "        ON p1.SUBJECT_ID = s.SUBJECT_ID AND p1.HADM_ID = s.HADM_ID \n",
    "        )\n",
    "        SELECT DIAGNOSIS, PHENOTYPE1, PHENOTYPE2, COUNT(*) AS N\n",
    "        FROM c\n",
    "        GROUP BY DIAGNOSIS, PHENOTYPE1, PHENOTYPE2\n",
    "    '''.format(phenotype), mydb)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createDiagnosisTable(diagnosis='428')\n",
    "diagnosisAndPhenotype1(phenotype='HP:0025547')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosisAndPhenotype2(phenotype='HP:0003259')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_all_brutal_v3():\n",
    "    summary_statistics1 = pd.DataFrame(data={'DIAGNOSIS_CODE':[], \n",
    "                       'P':[], \n",
    "                       'DIAGNOSIS':[], \n",
    "                       'PHENOTYPE':[], \n",
    "                       'N':[]},\n",
    "                columns = ['DIAGNOSIS_CODE', 'P', 'DIAGNOSIS', 'PHENOTYPE', 'N'])\n",
    "\n",
    "    summary_statistics2 = pd.DataFrame(data={'DIAGNOSIS_CODE':[], \n",
    "                       'P1':[], \n",
    "                       'P2':[], \n",
    "                       'DIAGNOSIS':[], \n",
    "                       'PHENOTYPE1':[], \n",
    "                       'PHENOTYPE2':[], \n",
    "                       'N':[]},\n",
    "                columns = ['DIAGNOSIS_CODE', 'P1', 'P2', 'DIAGNOSIS', 'PHENOTYPE1', 'PHENOTYPE2', 'N'])\n",
    "    diagnosisSet = diagnosis_set()\n",
    "    createAbnormalPhenotypeTable(1, force_update=True)\n",
    "    N = selectEncounters(force_update=True)\n",
    "    #for diagnosis in diagnosisSet.keys():\n",
    "    for diagnosis in ['428']:\n",
    "        if (diagnosisSet[diagnosis] > 10000):\n",
    "        #/*This is abnormal phenotypes. It is possible that one */\n",
    "            createDiagnosisTable(diagnosis)\n",
    "            pl = phenolist(diagnosis)\n",
    "            phenoSet = pl[pl['N'] > 1000].reset_index().MAP_TO\n",
    "            # compute the mutual information of each phenotype and diagnosis\n",
    "            for i in np.arange(len(phenoSet)):\n",
    "                phenotype1 = phenoSet[i]\n",
    "                result1 = diagnosisAndPhenotype1(phenotype1)\n",
    "                result1['DIAGNOSIS_CODE']= diagnosis\n",
    "                result1['P'] = phenotype1\n",
    "                summary_statistics1 = summary_statistics1.append(result1)\n",
    "                for j in np.arange(i + 1, len(phenoSet)):\n",
    "                    phenotype2 = phenoSet[j]\n",
    "                    result2 = diagnosisAndPhenotype2(phenotype2)\n",
    "                    result2['DIAGNOSIS_CODE']= diagnosis\n",
    "                    result2['P1'] = phenotype1\n",
    "                    result2['P2'] = phenotype2\n",
    "                    summary_statistics2 = summary_statistics2.append(result2)\n",
    "                    print('finishing phenotype1 ={}, phenotype2={}'.format(phenotype1, phenotype2))    \n",
    "    return N, summary_statistics1, summary_statistics2                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This improved method takes about 30 min - 1 hour to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "N, s1, s2 = iterate_all_brutal_v3() \n",
    "end = datetime.datetime.now()\n",
    "print('running time: {}s'.format((end - start).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.to_csv('summary_statistics1.csv', index = False)\n",
    "s2.to_csv('summary_statistics2.csv', index = False)\n",
    "print('total encounters analyzed: {}'.format(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = pd.read_csv('summary_statistics1.csv')\n",
    "s2 = pd.read_csv('summary_statistics2.csv')\n",
    "s1 = s1.loc[s1.DIAGNOSIS_CODE.astype('str') == '428', :]\n",
    "s2 = s2.loc[s2.DIAGNOSIS_CODE.astype('str') == '428', :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This should be returned from SQL calculations\n",
    "TOTAL_ENCOUNTER = N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy(x):\n",
    "    '''\n",
    "    Given the distribution of a random variable, return the entropy\n",
    "    '''\n",
    "    T = np.sum(x)\n",
    "    p = x / T\n",
    "    p_log = [0 if x == 0 else np.log2(x) for x in p]\n",
    "    return np.sum(p * p_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1 = s1.loc[:, ['PHENOTYPE', 'P', 'N']] \\\n",
    "    .groupby(['P', 'PHENOTYPE']) \\\n",
    "    .agg({'N': lambda x: sum(x) / TOTAL_ENCOUNTER}) \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={'N': 'FP'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2 = s1.loc[:, ['DIAGNOSIS', 'P', 'N']] \\\n",
    "    .groupby(['P', 'DIAGNOSIS']) \\\n",
    "    .agg({'N': lambda x: sum(x) / TOTAL_ENCOUNTER}) \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={'N': 'DP'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f3 = s1\n",
    "f3['prob'] = f3.N / TOTAL_ENCOUNTER\n",
    "f3 = f3.merge(f1, how = 'left', on = ['P', 'PHENOTYPE']).merge(f2, how = 'left', on = ['P', 'DIAGNOSIS'])\n",
    "f3['E'] = f3.prob * np.log2(f3.prob / (f3.FP * f3.DP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I = f3.groupby(['P']).agg({'E': sum}).reset_index().rename(columns={'E':'I'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F1 = s2.groupby(['P1', 'P2', 'PHENOTYPE1', 'PHENOTYPE2']) \\\n",
    "    .agg({'N':lambda x: sum(x) / TOTAL_ENCOUNTER}) \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={'N':'FP'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F2 = s2.groupby(['P1', 'P2', 'DIAGNOSIS']) \\\n",
    "    .agg({'N':lambda x: sum(x) / TOTAL_ENCOUNTER}) \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={'N':'DP'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F3 = s2\n",
    "F3['prob'] = F3.N / TOTAL_ENCOUNTER\n",
    "F3 = F3.merge(F1, how = 'left', on = ['P1', 'P2', 'PHENOTYPE1', 'PHENOTYPE2']) \\\n",
    "    .merge(F2, how = 'left', on = ['P1', 'P2', 'DIAGNOSIS'] )\n",
    "F3['E'] = F3.prob * np.log2(F3.prob / (F3.FP * F3.DP))\n",
    "F3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I2 = F3.groupby(['P1', 'P2']).agg({'E': sum}).reset_index().rename(columns={'E':'II'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "syn = I2.merge(I, how = 'left', left_on = ['P1'], right_on = 'P') \\\n",
    "        .rename(columns = {'I': 'I1'}) \\\n",
    "        .merge(I, how = 'left', left_on = ['P2'], right_on = 'P') \\\n",
    "        .rename(columns = {'I': 'I2'}) \\\n",
    "        .loc[:, ['P1', 'P2', 'II', 'I1', 'I2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "syn['syn'] = syn['II'] - syn['I1'] - syn['I2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn.sort_values(by = 'syn', ascending = False).head(n = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(syn.syn, bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Method 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This method relies on the power of MySQL for doing queies and joins, return a batch of phenotype profiles a time, and then use the power of Numpy to do numeric computation.\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "    1. For one diagnosis code, specify the phenotypes to analyze--a list of HPO terms.\n",
    "    2. For a batch of patient*encounters, return a list of diagnosis codes (1 or 0)\n",
    "    3. For the same batch of patient*encounters, return a list of phenotypes.\n",
    "    4. Create a numpy array with dimension (N x P)\n",
    "    5. Perform numeric computation with Numpy:\n",
    "        outer product for ++ of PxP.T\n",
    "        outer product for +- of Px(1-P).T\n",
    "        outer product for -+ of (1-P)xP\n",
    "        outer product for -- of (1-P)x(1-P).T\n",
    "        combine the above with - and + of diagnosis value\n",
    "        stack them together as a (N x P x P x 8) matrix.\n",
    "        Step 1 - 5 are performed at each site. The resulting matrix is returned to JAX for final analyze.\n",
    "    6. Compute pairwise synergy:\n",
    "        use the multi-dimension array to calculate p(D = 1), p(D = 0), p(P1 * P2)\n",
    "        compute mutual information of each phenotype in regarding to one diagnosis I(P:D)\n",
    "        compute mutual information of two phenotypes in regarding to one diagnosis I(P:D)\n",
    "        compute pairwise synergy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: rewrite to be backward compatible\n",
    "def encountersWithDiagnosis(diagnosis):\n",
    "    cursor.execute('''DROP TEMPORARY TABLE IF EXISTS d''')\n",
    "    cursor.execute('''\n",
    "        CREATE TEMPORARY TABLE IF NOT EXISTS d\n",
    "        SELECT \n",
    "            DISTINCT SUBJECT_ID, HADM_ID, 1 AS DIAGNOSIS\n",
    "        FROM \n",
    "            DIAGNOSES_ICD \n",
    "        WHERE ICD9_CODE LIKE '{}%'\n",
    "        -- This is encounters with positive diagnosis\n",
    "    '''.format(diagnosis))\n",
    "    cursor.execute('CREATE INDEX d_idx01 ON d(SUBJECT_ID, HADM_ID)')\n",
    "\n",
    "    \n",
    "def createPhenotypeSet(diagnosis, threshold=1000):\n",
    "    \"\"\"\n",
    "    Create the phenotypes that we should analyze. Exemely less frequently observed phenotypes are excluded.\n",
    "    \"\"\"\n",
    "    cursor.execute('DROP TEMPORARY TABLE IF EXISTS ps')\n",
    "    cursor.execute('''\n",
    "            CREATE TEMPORARY TABLE ps\n",
    "            WITH pd AS(\n",
    "                SELECT p.*\n",
    "                FROM \n",
    "                    p JOIN (SELECT \n",
    "                                DISTINCT SUBJECT_ID, HADM_ID, 1 AS DIAGNOSIS\n",
    "                            FROM \n",
    "                                DIAGNOSES_ICD \n",
    "                            WHERE ICD9_CODE LIKE '{}%') AS d\n",
    "                    ON p.SUBJECT_ID = d.SUBJECT_ID AND p.HADM_ID = d.HADM_ID)\n",
    "            SELECT \n",
    "                MAP_TO, COUNT(*) AS N, 1 AS PHENOTYPE\n",
    "            FROM pd\n",
    "            GROUP BY MAP_TO\n",
    "            HAVING N > {}\n",
    "            ORDER BY N DESC'''.format(diagnosis, threshold))\n",
    "    phenoSet = pd.read_sql_query('SELECT * FROM ps', mydb)\n",
    "    return phenoSet\n",
    "\n",
    "\n",
    "def batch_query(start_index, end_index):\n",
    "    cursor.execute('''\n",
    "                SELECT \n",
    "                    COUNT(DISTINCT SUBJECT_ID, HADM_ID) \n",
    "                FROM admissions \n",
    "                WHERE SUBJECT_ID BETWEEN {} AND {}\n",
    "                '''.format(start_index, end_index))\n",
    "    batch_size_actual = cursor.fetchall()[0][0]\n",
    "    # create diagnosis table\n",
    "    diagnosisList = pd.read_sql_query('''\n",
    "                WITH a AS (\n",
    "                    SELECT DISTINCT SUBJECT_ID, HADM_ID \n",
    "                    FROM admissions \n",
    "                    WHERE SUBJECT_ID BETWEEN {} AND {})\n",
    "                SELECT \n",
    "                    a.SUBJECT_ID, a.HADM_ID, IF(d.DIAGNOSIS IS NULL, 0, 1) AS DIAGNOSIS\n",
    "                FROM \n",
    "                    a\n",
    "                LEFT JOIN\n",
    "                    d ON a.SUBJECT_ID = d.SUBJECT_ID AND a.HADM_ID = d.HADM_ID         \n",
    "                '''.format(start_index, end_index), mydb)\n",
    "    # create phenotype profile table\n",
    "    phenotyle_profile = pd.read_sql_query('''\n",
    "        WITH \n",
    "            a AS (\n",
    "                    SELECT \n",
    "                        DISTINCT SUBJECT_ID, HADM_ID \n",
    "                    FROM \n",
    "                        admissions \n",
    "                    WHERE SUBJECT_ID BETWEEN {} AND {}), \n",
    "            c as (\n",
    "                SELECT a.*, ps.MAP_TO\n",
    "                FROM a\n",
    "                JOIN ps),\n",
    "                -- cross product of all patient*encounter and phenotypes list\n",
    "            pp as (\n",
    "                SELECT p.*, 1 AS PHENOTYPE \n",
    "                FROM p RIGHT JOIN a \n",
    "                ON p.SUBJECT_ID = a.SUBJECT_ID AND p.HADM_ID = a.HADM_ID)\n",
    "\n",
    "        SELECT c.SUBJECT_ID, c.HADM_ID, c.MAP_TO, IF(pp.PHENOTYPE IS NULL, 0, 1) AS PHENOTYPE \n",
    "        FROM pp \n",
    "        RIGHT JOIN c ON pp.SUBJECT_ID = c.SUBJECT_ID and pp.HADM_ID = c.HADM_ID AND pp.MAP_TO = c.MAP_TO\n",
    "        '''.format(start_index, end_index), mydb)\n",
    "    return batch_size_actual, diagnosisList, phenotyle_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_in_batch(logger):\n",
    "    logger.info('starting iterate_in_batch()')\n",
    "    batch_size = 100\n",
    "    # find the set of diagnosis that are worthy to analyze\n",
    "    diagnosisSet = diagnosis_set()\n",
    "    logger.info('diagnosis set completed')\n",
    "\n",
    "    # create a temp table for abnormal phenotypes of each patient*encounter that met the threshold\n",
    "    createAbnormalPhenotypeTable(threshold=1, force_update=True)\n",
    "    logger.info('createAbnormalPhenotypeTable() completed')\n",
    "    \n",
    "    synergies = {}\n",
    "    \n",
    "    for diagnosis in diagnosisSet.keys():\n",
    "        if (diagnosisSet[diagnosis] > 10000):\n",
    "            # create a temp table for diagnosis of all patient*encouter to analyze\n",
    "            encountersWithDiagnosis(diagnosis)\n",
    "            logger.info('encountersWithDiagnosis() completed')\n",
    "\n",
    "            ## create a list of phenotypes that we want to analyze for the specified disease and preset threshold\n",
    "            phenoSet = createPhenotypeSet(diagnosis, threshold=100)\n",
    "            logger.info('phenoSet completed')\n",
    "            P_SIZE = len(phenoSet)\n",
    "\n",
    "            ## find the start and end ROW_ID for patient*encounter\n",
    "            cursor.execute('SELECT MIN(ROW_ID) AS min, MAX(ROW_ID) AS max FROM admissions')\n",
    "            ADM_ID_START, ADM_ID_END = cursor.fetchall()[0]\n",
    "            batch_N = ADM_ID_END - ADM_ID_START + 1\n",
    "            TOTAL_BATCH = math.ceil(batch_N / batch_size) # total number of batches\n",
    "            synergies[diagnosis] = mf.Synergy(diagnosis, phenoSet.MAP_TO)\n",
    "            logger.info('starting batch queries for {}'.format(diagnosis))\n",
    "            for i in np.arange(TOTAL_BATCH):\n",
    "                start_index = i * batch_size + ADM_ID_START\n",
    "                if i < TOTAL_BATCH - 1:\n",
    "                    end_index = start_index + batch_size - 1\n",
    "                else:\n",
    "                    end_index = batch_N\n",
    "                \n",
    "                batch_size_actual, diagnosisList, phenotyle_profile = batch_query(start_index, end_index)\n",
    "                \n",
    "                if batch_size_actual > 0 :\n",
    "                    diagnosisVector = diagnosisList.DIAGNOSIS\n",
    "                    phenotypeProfileMatrix = phenotyle_profile.PHENOTYPE.values.reshape([batch_size_actual, P_SIZE])\n",
    "                    if i % 10000 == 0:\n",
    "                        logger.info('new batch: start_index={}, end_index={}, batch_size= {}, phenotype_size = {}'.format(start_index, end_index, batch_size_actual, len(phenoSet)))\n",
    "                    synergies[diagnosis].add_batch(phenotypeProfileMatrix, diagnosisVector)\n",
    "    \n",
    "    return synergies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method takes only 2 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s',level=logging.DEBUG, stream=sys.stdout)\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "synergies = iterate_in_batch(logger)\n",
    "   \n",
    "end = datetime.datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time: 910.764335s\n"
     ]
    }
   ],
   "source": [
    "print('running time: {}s'.format((end - start).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I, II = synergies['428'].mutual_information()\n",
    "S = synergies['428'].pairwise_synergy()\n",
    "np.save('I_July24', I)\n",
    "np.save('II_July24', II)\n",
    "np.save('S_July24', S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('phenotype_list', synergies['428'].get_phenotype_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.11314247e-03 -4.11314247e-03 -1.53254389e-03 ...  4.67180356e-04\n",
      "  -1.94397091e-04 -2.83708740e-04]\n",
      " [-4.11314247e-03 -4.11314247e-03 -1.53254389e-03 ...  4.67180356e-04\n",
      "  -1.94397091e-04 -2.83708740e-04]\n",
      " [-1.53254389e-03 -1.53254389e-03 -2.37888692e-02 ...  2.03364556e-03\n",
      "  -6.66431483e-04 -1.05061849e-03]\n",
      " ...\n",
      " [ 4.67180356e-04  4.67180356e-04  2.03364556e-03 ... -3.25355943e-03\n",
      "   2.46001591e-03  2.39288817e-04]\n",
      " [-1.94397091e-04 -1.94397091e-04 -6.66431483e-04 ...  2.46001591e-03\n",
      "  -1.13447141e-03 -3.05967382e-05]\n",
      " [-2.83708740e-04 -2.83708740e-04 -1.05061849e-03 ...  2.39288817e-04\n",
      "  -3.05967382e-05 -3.10072696e-03]]\n"
     ]
    }
   ],
   "source": [
    "S = np.load('S_July24.npy')\n",
    "label = np.load('phenotype_list.npy')\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6hgOkqSO4SBJ6hgOkqSO4SBJ6vx/aEkID9Z6fx0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111695d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(S.flat, bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangx/anaconda3/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py:150: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if string == 'category':\n"
     ]
    }
   ],
   "source": [
    "N = len(label)\n",
    "P1 = np.repeat(label, N)\n",
    "P2 = np.tile(label, N)\n",
    "data = pd.DataFrame(data = {'P1': P1, 'P2': P2, 'synergy': S.flat})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('synergies.obj', 'wb') as synergies_file:\n",
    "    pickle.dump(synergies, synergies_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('synergies.obj', 'rb') as synergies_file:\n",
    "    deserialized = pickle.load(synergies_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(mf_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the synergy scores. They are all pretty small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11481fc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(heart_failure.pairwise_synergy().flat, bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangx/anaconda3/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py:150: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if string == 'category':\n"
     ]
    }
   ],
   "source": [
    "label = heart_failure.get_phenotype_list()\n",
    "S = heart_failure.pairwise_synergy()\n",
    "N = len(label)\n",
    "P1 = np.repeat(label, N)\n",
    "P2 = np.tile(label, N)\n",
    "data = pd.DataFrame(data = {'P1': P1, 'P2': P2, 'synergy': S.flat})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove phenotype pairs of which one is the ancestor or descendent of the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hpo = hpoutil.HPO('/Users/zhangx/git/human-phenotype-ontology/hp.obo')\n",
    "mask = np.array([hpo.has_dependency(data.P1[i], data.P2[i]) for i in np.arange(data.shape[0])])\n",
    "S_heart_failure = data.loc[np.logical_not(mask), :].sort_values(by = 'synergy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove duplication record: (HP1, HP2) is the same to (HP2, HP1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S_heart_failure = S_heart_failure.loc[S_heart_failure.P1 < S_heart_failure.P2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5percent = S_heart_failure.shape[0] * 0.05\n",
    "\n",
    "top5_synergy_pair_heart_failure = S_heart_failure.reset_index(drop=True).iloc[0:int(top5percent), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top5_synergy_pair_heart_failure['P1_label'] = np.array([hpo.term_id2name_map().get(termid) for termid in top5_synergy_pair_heart_failure.P1])\n",
    "top5_synergy_pair_heart_failure['P2_label'] = np.array([hpo.term_id2name_map().get(termid) for termid in top5_synergy_pair_heart_failure.P2])\n",
    "top5_synergy_pair_heart_failure.to_csv('top5_synergy_pair_heart_failure.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>synergy</th>\n",
       "      <th>P1_label</th>\n",
       "      <th>P2_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HP:0002904</td>\n",
       "      <td>HP:0012379</td>\n",
       "      <td>0.009951</td>\n",
       "      <td>Hyperbilirubinemia</td>\n",
       "      <td>Abnormal enzyme/coenzyme activity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HP:0003573</td>\n",
       "      <td>HP:0012379</td>\n",
       "      <td>0.009292</td>\n",
       "      <td>Increased total bilirubin</td>\n",
       "      <td>Abnormal enzyme/coenzyme activity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HP:0004364</td>\n",
       "      <td>HP:0031969</td>\n",
       "      <td>0.007833</td>\n",
       "      <td>Abnormal circulating nitrogen compound concent...</td>\n",
       "      <td>Reduced blood urea nitrogen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HP:0002012</td>\n",
       "      <td>HP:0002904</td>\n",
       "      <td>0.007525</td>\n",
       "      <td>Abnormality of the abdominal organs</td>\n",
       "      <td>Hyperbilirubinemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HP:0001392</td>\n",
       "      <td>HP:0002904</td>\n",
       "      <td>0.007525</td>\n",
       "      <td>Abnormality of the liver</td>\n",
       "      <td>Hyperbilirubinemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HP:0002904</td>\n",
       "      <td>HP:0410042</td>\n",
       "      <td>0.007525</td>\n",
       "      <td>Hyperbilirubinemia</td>\n",
       "      <td>Abnormal liver morphology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HP:0002904</td>\n",
       "      <td>HP:0002910</td>\n",
       "      <td>0.007525</td>\n",
       "      <td>Hyperbilirubinemia</td>\n",
       "      <td>Elevated hepatic transaminase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HP:0002904</td>\n",
       "      <td>HP:0025031</td>\n",
       "      <td>0.007525</td>\n",
       "      <td>Hyperbilirubinemia</td>\n",
       "      <td>Abnormality of the digestive system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HP:0020058</td>\n",
       "      <td>HP:0025548</td>\n",
       "      <td>0.007221</td>\n",
       "      <td>Abnormal red blood cell count</td>\n",
       "      <td>Increased mean corpuscular hemoglobin concentr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HP:0003573</td>\n",
       "      <td>HP:0410042</td>\n",
       "      <td>0.007209</td>\n",
       "      <td>Increased total bilirubin</td>\n",
       "      <td>Abnormal liver morphology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HP:0003573</td>\n",
       "      <td>HP:0025031</td>\n",
       "      <td>0.007209</td>\n",
       "      <td>Increased total bilirubin</td>\n",
       "      <td>Abnormality of the digestive system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HP:0001392</td>\n",
       "      <td>HP:0003573</td>\n",
       "      <td>0.007209</td>\n",
       "      <td>Abnormality of the liver</td>\n",
       "      <td>Increased total bilirubin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HP:0002910</td>\n",
       "      <td>HP:0003573</td>\n",
       "      <td>0.007209</td>\n",
       "      <td>Elevated hepatic transaminase</td>\n",
       "      <td>Increased total bilirubin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HP:0002012</td>\n",
       "      <td>HP:0003573</td>\n",
       "      <td>0.007209</td>\n",
       "      <td>Abnormality of the abdominal organs</td>\n",
       "      <td>Increased total bilirubin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HP:0002904</td>\n",
       "      <td>HP:0031956</td>\n",
       "      <td>0.007159</td>\n",
       "      <td>Hyperbilirubinemia</td>\n",
       "      <td>Elevated serum aspartate aminotransferase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>HP:0002904</td>\n",
       "      <td>HP:0031965</td>\n",
       "      <td>0.007111</td>\n",
       "      <td>Hyperbilirubinemia</td>\n",
       "      <td>Increased RBC distribution width</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>HP:0003573</td>\n",
       "      <td>HP:0031965</td>\n",
       "      <td>0.006872</td>\n",
       "      <td>Increased total bilirubin</td>\n",
       "      <td>Increased RBC distribution width</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>HP:0003573</td>\n",
       "      <td>HP:0031956</td>\n",
       "      <td>0.006822</td>\n",
       "      <td>Increased total bilirubin</td>\n",
       "      <td>Elevated serum aspartate aminotransferase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>HP:0002904</td>\n",
       "      <td>HP:0010876</td>\n",
       "      <td>0.006517</td>\n",
       "      <td>Hyperbilirubinemia</td>\n",
       "      <td>Abnormal circulating protein level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>HP:0002904</td>\n",
       "      <td>HP:0045040</td>\n",
       "      <td>0.006420</td>\n",
       "      <td>Hyperbilirubinemia</td>\n",
       "      <td>Abnormal lactate dehydrogenase activity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            P1          P2   synergy  \\\n",
       "0   HP:0002904  HP:0012379  0.009951   \n",
       "1   HP:0003573  HP:0012379  0.009292   \n",
       "2   HP:0004364  HP:0031969  0.007833   \n",
       "3   HP:0002012  HP:0002904  0.007525   \n",
       "4   HP:0001392  HP:0002904  0.007525   \n",
       "5   HP:0002904  HP:0410042  0.007525   \n",
       "6   HP:0002904  HP:0002910  0.007525   \n",
       "7   HP:0002904  HP:0025031  0.007525   \n",
       "8   HP:0020058  HP:0025548  0.007221   \n",
       "9   HP:0003573  HP:0410042  0.007209   \n",
       "10  HP:0003573  HP:0025031  0.007209   \n",
       "11  HP:0001392  HP:0003573  0.007209   \n",
       "12  HP:0002910  HP:0003573  0.007209   \n",
       "13  HP:0002012  HP:0003573  0.007209   \n",
       "14  HP:0002904  HP:0031956  0.007159   \n",
       "15  HP:0002904  HP:0031965  0.007111   \n",
       "16  HP:0003573  HP:0031965  0.006872   \n",
       "17  HP:0003573  HP:0031956  0.006822   \n",
       "18  HP:0002904  HP:0010876  0.006517   \n",
       "19  HP:0002904  HP:0045040  0.006420   \n",
       "\n",
       "                                             P1_label  \\\n",
       "0                                  Hyperbilirubinemia   \n",
       "1                           Increased total bilirubin   \n",
       "2   Abnormal circulating nitrogen compound concent...   \n",
       "3                 Abnormality of the abdominal organs   \n",
       "4                            Abnormality of the liver   \n",
       "5                                  Hyperbilirubinemia   \n",
       "6                                  Hyperbilirubinemia   \n",
       "7                                  Hyperbilirubinemia   \n",
       "8                       Abnormal red blood cell count   \n",
       "9                           Increased total bilirubin   \n",
       "10                          Increased total bilirubin   \n",
       "11                           Abnormality of the liver   \n",
       "12                      Elevated hepatic transaminase   \n",
       "13                Abnormality of the abdominal organs   \n",
       "14                                 Hyperbilirubinemia   \n",
       "15                                 Hyperbilirubinemia   \n",
       "16                          Increased total bilirubin   \n",
       "17                          Increased total bilirubin   \n",
       "18                                 Hyperbilirubinemia   \n",
       "19                                 Hyperbilirubinemia   \n",
       "\n",
       "                                             P2_label  \n",
       "0                   Abnormal enzyme/coenzyme activity  \n",
       "1                   Abnormal enzyme/coenzyme activity  \n",
       "2                         Reduced blood urea nitrogen  \n",
       "3                                  Hyperbilirubinemia  \n",
       "4                                  Hyperbilirubinemia  \n",
       "5                           Abnormal liver morphology  \n",
       "6                       Elevated hepatic transaminase  \n",
       "7                 Abnormality of the digestive system  \n",
       "8   Increased mean corpuscular hemoglobin concentr...  \n",
       "9                           Abnormal liver morphology  \n",
       "10                Abnormality of the digestive system  \n",
       "11                          Increased total bilirubin  \n",
       "12                          Increased total bilirubin  \n",
       "13                          Increased total bilirubin  \n",
       "14          Elevated serum aspartate aminotransferase  \n",
       "15                   Increased RBC distribution width  \n",
       "16                   Increased RBC distribution width  \n",
       "17          Elevated serum aspartate aminotransferase  \n",
       "18                 Abnormal circulating protein level  \n",
       "19            Abnormal lactate dehydrogenase activity  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_synergy_pair_heart_failure.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primary_diagnosis_count = pd.read_sql_query('''SELECT \n",
    "                        ICD9_CODE, COUNT(*) AS N \n",
    "                    FROM \n",
    "                        DIAGNOSES_ICD \n",
    "                    WHERE \n",
    "                        SEQ_NUM = 1 \n",
    "                    GROUP BY \n",
    "                        ICD9_CODE \n",
    "                    HAVING \n",
    "                        N > 100 \n",
    "                    ORDER BY \n",
    "                        N DESC ;\n",
    "''', mydb)\n",
    "primary_diagnosis_count.to_csv('primary_diagnosis_count.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor.close()\n",
    "mydb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
